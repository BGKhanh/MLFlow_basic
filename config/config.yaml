# Dataset configuration
dataset:
  name: "cifar10"  # Options: "cifar10", "cifar100"
  data_dir: "data/"
  img_size: 32  # This will be automatically adjusted based on the model if not specified
  num_classes: 10  # This will be automatically set based on the dataset name
  num_workers: 2  # Number of workers for data loading

# Model configuration
model:
  name: "mobilenet"  # Options: "mobilenet", "efficientnet", "densenet", "regnet"
  transfer_mode: "classifier_only"

# Training configuration
training:
  batch_size: 128
  num_epochs: 10
  checkpoint_dir: "checkpoints"
  optimizer: "adamw"  # Only AdamW is allowed
  learning_rate: 0.0001
  weight_decay: 0.01
  # Scheduler options
  scheduler: "cosine"  # Options: "step", "cosine", "none"
  step_size: 10
  gamma: 0.1
  early_stopping: true
  patience: 5
  seed: 42
  # Mixed precision training
  use_amp: true  # Enable automatic mixed precision training

# Augmentation configuration
augmentation:
  use_augmentation: true
  # Basic transformations
  random_resized_crop: true
  random_crop: true
  horizontal_flip: true
  vertical_flip: true
  # Rotations
  rotation_degrees: 15
  square_symmetry_rotations: true  # 90/180/270 degree rotations
  # Dropouts
  coarse_dropout: true
  coarse_dropout_max_holes: 8
  coarse_dropout_max_height: 8
  coarse_dropout_max_width: 8
  coarse_dropout_min_holes: 1
  channel_dropout: true
  # Color adjustments
  color_jitter: true
  brightness: 0.2
  contrast: 0.2
  saturation: 0.2
  hue: 0.1
  # Geometric transformations
  affine: true
  affine_scale: [0.8, 1.2]
  affine_rotate: [-15, 15]
  # Robustness methods
  noise: true
  blur: true
  # Additional augmentations
  grid_distortion: true  # GridDistortion transformation
  elastic_transform: true  # ElasticTransform for flexible distortions
  cutout: true  # Alternative to CoarseDropout
  grid_mask: true  # GridDropout method

# MLflow configuration
mlflow:
  tracking_uri: "./mlruns" # Thư mục lưu trữ kết quả. Đổi sang sever nếu dự án nhiều người. vd "http://localhost:5000"
  experiment_name: "cifar_classification"
  log_model: true
  register_model: false
  model_name: "cifar_classifier"

# Ray Tune configuration
ray_tune:
  use_ray_tune: true
  num_samples: 10
  resources_per_trial:
    cpu: 2
    gpu: 0 #Change to the number of GPUs you want to use
  parameters:
    optimizer:
      values: ["adamw"]
    learning_rate:
      min: 0.0001
      max: 0.01
    weight_decay:
      min: 0.0001
      max: 0.1
    batch_size:
      values: [32, 64, 128]
    dropout_rate:
      min: 0.1
      max: 0.5

# Distributed training configuration
distributed:
  use_distributed: true
  world_size: 1    # Should be chosen based on device state
  backend: "gloo"  # Options: "gloo", "nccl". "nccl" for GPU usecase
  use_gpu: false
  init_method: "tcp://localhost:23456" 